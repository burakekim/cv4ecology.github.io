<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Students</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-DCEY6Y6YPV"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-DCEY6Y6YPV');
		</script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>CV4Ecology Workshop</strong></a>
									<ul class="icons">
										<li><a href="https://twitter.com/cv4ecology" target="_blank" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="https://www.youtube.com/@cv4ecology" target="_blank" class="icon brands fa-youtube"><span class="label">Twitter</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section><!--style="margin-top: -50px;">-->
									<header class="main">
										<h1>Students 2025</h1>
									</header>
									<span class="image main"><img src="images/fish2023banner.png" alt="" /></span>
									<hr />
									<div>
										<span>
										  <h3 style="display: inline;">Students by year: </h3> 
										  [<a href="students2022.html">2022</a>] 
										  [<a href="students2023.html">2023</a>] 
										  [<a href="students2025.html">2025</a>] 
										  <!--[<a href="speakers2025.html">2025</a>]-->
										</span>
									  </div>
									<hr />
									<div class="bio-container">
										<span class="image left">
											<img src="images/student_luypaert.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Thomas Luypaert (<a href="https://twitter.com/LuypaertThomas" target="_blank">@LuypaertThomas</a>)</h4>
										<i>Norwegian University of Life Sciences</i>

										<p class="bio" style="margin-top: 10px;">
											I am Thomas, a tropical ecologist, and current postdoctoral researcher at the University of Life Sciences. My research lies at the interface of tropical ecology and big data science, and has the overarching aim of alleviating barriers that hold back large-scale biodiversity monitoring in tropical rainforest ecosystems. During my PhD, I developed an analytical pipeline for tracking biodiversity changes in rainforests using large acoustic datasets, without needing species-level identification. Currently, my research explores ways to extract ecologically important information from eDNA, camera traps, acoustic recordings, and visual surveys collected during expeditions across the Brazilian Amazon.
										</p>
										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> As part of the Amazon Biodiversity & Carbon Expeditions (abc-expeditions.com), our team has collected approximately 45,000 camera trap videos capturing large vertebrates throughout the Brazilian Amazon. Through the CV4Ecology project, I hope to identify a workable machine learning model to detect animals and classify the most commonly observed species. Our goal is to create a scalable, generalizable algorithm that reduces the time spent sorting non-target footage (e.g., blank images) while accurately identifying common species, without the need for regional retraining.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_ford.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Brett Ford</h4>
										<i>North Country Wild project at St. Lawrence University</i>

										<p class="bio" style="margin-top: 10px;">
											I’m a developer with the North Country Wild project at St. Lawrence University. I work as a bioinformatics scientist during the day and volunteer my time to the North Country Wild project to help manage and process camera trap and passive acoustic monitoring data. I completed my MSc at the University of British Columbia and my BS at St. Lawrence University. I’m excited about combining my experience in programming and data science with my background in ecology and evolution to help inform biodiversity monitoring efforts.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Maintaining connectivity between large swaths of natural habitat is essential for the maintenance of biodiversity, however we still lack a fundamental understanding of what species live where and how animals use the landscape. As part of the Algonquin to Adirondack (A2A) Collaborative, the North Country Wild project has deployed camera traps and passive acoustic monitoring devices to understand what species live within the A2A corridor and what habitats they most often occupy. While it’s relatively easy to solicit classification of game camera images through citizen science platforms like Zooniverse, acoustic monitoring files are more difficult to sift through and classify, needing more specialized knowledge of animal sounds and a longer amount of analysis time. Therefore, I am hoping to use recent advances in computer vision to help filter through the hours of amassed acoustic data with the intent of applying learnings from the models generated for the audio data as an additional validation method for the camera trap data. Ultimately, the data we retrieve by applying the computer vision methods will help inform critical habitat areas within the A2A corridor and will identify areas for preservation or restoration. 
										</p>
									</div>
									
									<div class="bio-container">
										<span class="image left">
											<img src="images/student_cressman.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Shannon Cressman</h4>
										<i>United States Fish and Wildlife Service- Green Bay Fish and Wildlife Conservation Office</i>

										<p class="bio" style="margin-top: 10px;">
											Hi, I’m Shannon Cressman, a lead biological science technician for the USFWS in Green Bay, Wisconsin. My job entails surveying angler caught salmonids that reside in Lake Michigan and Lake Huron of the Great Lakes. I have my B.S. in Marine Biology with a concentration in biological sciences from the University of Maine at Machias. 
										</p>
										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Our program, the Great Lakes Fish Tagging and Recovery lab ages over 3,500 structures from both hatchery and wild-caught salmon and trout. Specifically, we analyze scales from Chinook Salmon and Steelhead, as well as otoliths from Lake Trout. These structures are aged within three months during the winter, leading to a significant workload. To address this, I aim to create a program using AI to detect annular rings in aging structures to estimate age. This will streamline the aging process and ensure consistency among agers across state, federal, and tribal agencies throughout the Great Lakes region.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_ferrari.JPG" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Nina Ferrari</h4>
										 <i>Oregon State University</i> 

										<p class="bio" style="margin-top: 10px;">
											I'm a PhD student at Oregon State University working with Dr. Matt Betts and Dr. Erica Fleishman. My research aims to answer central questions of species coexistence theory and niche partitioning. I investigate the abiotic (microclimate) and biotic (competition and vegetation structure) drivers of vertical bird distributions in old-growth and second-growth forests. I utilize myriad techniques including tree climbing, machine learning of bioacoustics, and lidar to evaluate fine-resolution metrics across the vertical dimension in the H.J. Andrews Experimental Forest. I am particularly interested in how birds can mitigate climate driven changes by modifying their habitat use.										
										</p>
										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> The use of ARUs and bioacoustics is increasing in the field of avian ecology. However, most studies use data at coarse horizontal spatial resolutions to answer questions about population dynamics and site occupancy. I am interested in using bioacoustics data to answer questions about fine-resolution behavior and vertical habitat use of birds.  I will build a multi-species classifier that can predict the height in a tree a vocalization occurs based on relative decibel levels across multiple ARUs. A goal this project is to better understand how different forest management practices (harvest regimes of second-growth stands, retention of old-growth trees) influence vertical habitat use by forest songbirds.
										</p>
									</div>
									
									<div class="bio-container">
										<span class="image left">
											<img src="images/student_grabowski.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Katie Grabowski</h4>
										<i>University of British Columbia</i>

										<p class="bio" style="margin-top: 10px;">
											I'm Katie, a first year PhD student at the University of British Columbia in Vancouver, Canada. For my PhD, I am working in Gorongosa National Park (GNP) in Mozambique, studying the effects of large carnivore reintroduction on the mesocarnivore community. Camera traps play a vital role in collecting information about these small, elusive nocturnal animals, and I am interested in leveraging the power of computer vision to help process the collected data more efficiently. Outside of my research, you can find me running, swimming, baking, or hanging out with my dog, Charlie. 
										</p>
										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> The widespread global loss of apex predators from ecosystems has led to dramatic changes in downstream trophic levels, including the release of smaller-bodied mesocarnivores. As apex predators are reintroduced to ecosystems where they were extirpated, it remains unknown how mesocarnivore populations will respond. For my project, I will use data from a camera trap grid in GNP that has been operating since 2016. My goal at CV4E is to train a model to identify the species and number of individuals for each image sequence, not only for mesocarnivores but for all large mammals, so that we may better monitor changes to the wildlife community in the wake of large carnivore reintroduction.  My ecological questions will benefit from the use of computer vision to speed the timeline from the detection of an animal by a camera trap to generating usable data for building models.
										</p>
									</div>
									
									<div class="bio-container">
										<span class="image left">
											<img src="images/student_digiacomo.jpeg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Alexandra DiGiacomo</h4>
										<i>Stanford University</i>

										<p class="bio" style="margin-top: 10px;">
											I am a PhD candidate in Biology at Stanford University. My dissertation research integrates remote sensing and biologging technology to investigate white shark movement ecology in Monterey Bay. Previously, I received a B.S. in Biology at Duke University and conducted research in Duke's Marine Robotics and Remote Sensing Laboratory.  As a scientist, I am broadly motivated to seek solutions that expand the scale at which we can understand and conserve ocean ecosystems. For this reason, my research works to leverage and develop computational approaches to answer critical biological and ecological questions.
										</p>

									<!---	<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> 
										</p>-->
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_ayad.JPG" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Mariam Ayad</h4>
										<i>University of California, Santa Cruz</i>

										<p class="bio" style="margin-top: 10px;">
											Mariam is a PhD candidate at the University of California, Santa Cruz in the Ocean Science Department. She graduated from California State University of Los Angeles with an M.S. in Environmental Science with an emphasis in Geospatial Sciences and from the University of California Irvine with a B.S. in Earth System Science. Her master’s thesis work was on remote sensing and classification techniques to identify different types of marine pollution, specifically utilizing remote sensing and differentiating stormwater and wastewater runoff in the Tijuana River. This work was published to Frontiers in Environmental Sciences in December 2020 (Ayad et al., 2020). She received the NASA MSI fellowship to support her PhD research: “Detection and Analysis of Coral Stressors in Florida Keys and Belize Barrier Reef using Remote Sensing Imagery”. The goal of this project is to identify high-risk areas that are contributing to coral reef bleaching or inhibiting coral growth. She is very interested in coral reefs and their response to various chemical and physical changes in the ocean and climate as a whole. She hopes to incorporate remote sensing and field measurements to quantify the response of coral reefs to a changing climate.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Coral reefs are vulnerable ecosystems threatened by anthropogenic stressors and climate change. About 40% of coral reefs have been lost in the past 40 years in the Caribbean Sea. Coral reefs face various stressors such as nutrient pollution, overfishing, sedimentation, increased sea surface temperatures (SST), and ocean acidification. I aim to develop a machine learning model with in-situ validation to identify healthy and bleached coral using PlanetScope SuperDove satellite imagery. This research would be of value to coral reef managers and stakeholders in terms of ocean sustainability projects, reef management practices, and long-term predictions of the ocean ecosystem.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_michel.JPG" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Alice Michel</h4>
										<i>University of California, Davis</i>

										<p class="bio" style="margin-top: 10px;">
											I am a PhD candidate at the University of California, Davis. I study intergroup interactions in western lowland gorillas. My research takes place in a community reserve in the peatland rainforest of northern Republic of Congo, where I use passive acoustic monitoring, genetics, and traditional field techniques to collect data alongside people from a local village. My hope is that the act of basic scientific research, aiming simply to ask questions about our world, will support better conservation outcomes in critical ecosystems. When I'm not searching for gorilla poop in the forest or gorilla sounds on the computer, I enjoy running, biking, and taking pictures.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> In some gorilla populations, adult male, or silverback, gorillas communicate with other groups by chest beating back and forth at night. With the end objective of facilitating my research on the form and function of this form of communication, the goal of this project is to develop computer vision methods to detect gorilla chest beats in five months of passive acoustic recordings. A subsequent goal is to determine if there is sufficient intraindividual repeatability and interindividual variability in chest beats with which to identify individual gorillas. 
										</p>
									</div>
									
									<div class="bio-container">
										<span class="image left">
											<img src="images/student_holmes.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Sheila Holmes</h4>
										<i>Swedish University of Agricultural Sciences</i>

										<p class="bio" style="margin-top: 10px;">
											I am a researcher at the department of Wildlife, Fish and Environmental Studies, Swedish University of Agricultural Sciences in Umeå, Sweden. My main research focus is measuring and understanding the impacts of human activities on various aspects of sustainable development. For example, I study how restoration activities in Madagascar’s humid forests affect biodiversity, human livelihoods, and zoonotic disease risk. I work closely with several academics and conservation NGOs, and I look forward to sharing the lessons I learn at the CV4E workshop with my students and collaborators, to support ongoing and future biodiversity monitoring efforts. 
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b>Several conservation NGOs in Madagascar have ongoing reforestation programs but lack the resources to monitor the biodiversity outcomes of these programs. Though there is interest in using acoustic recorders to assist in monitoring, current analysis tools perform poorly at identifying rare tropical species. During the CV4E workshop, I will work on training a multi-species machine learning model to identify seed dispersing birds from AudioMoth recordings in forest, reforestation areas, and cleared/burned areas across the humid forest ecoregion. We can then use detections in occupancy modelling to identify factors that support biodiversity and seed dispersal function in reforestation areas. 
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_demattei.jpeg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Braden Charles DeMattei</h4>
										<i>Carngie Science, Hampton Lab</i>

										<p class="bio" style="margin-top: 10px;">
											I am the lab and data manager for Dr. Stephanie Hampton’s lab at Carnegie Science where we study the ecology of both rotifers and lakes under ice. I am interested in developing and applying computer vision and machine learning techniques to food web ecology and species interaction analysis. I received my B.S. in Marine Biology from UCLA and my MSc in Marine Conservation at the University of Aberdeen with a focus on advanced statistics. I have worked on everything from coral reef ecology to Scottish demersal mixed fisheries management to California Central Valley salmon population management.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Applying computer vision to trophic interaction and food web ecology is something that has not yet been fully explored in the literature, as it is very difficult to collect enough data for most predator-prey pairs. Using the sessile rotifer, Rhinoglena sp., as the predator and the motile algae, Cryptomonas erosa, as prey, my project for CV4E will look at detecting when and how often a predator and its prey encounter each other. Rhinoglena remains stationary while it hunts and creates a feeding field with cilia to bring its prey to it. This makes it relatively easy to film the entirety of its hunting efforts without needing to refocus the microscope. Developing this algorithm will hopefully be the jumping off point for a grander project that will develop a predator-prey interaction detection and classification model that will allow for more in-depth analyses of freshwater food web ecology. 
										</p>
									</div>
							
						   
								<div class="bio-container">
										<span class="image left">
											<img src="images/student_zereshel.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Guy Zer Eshel</h4>
										<i>Tel Aviv University</i>

										<p class="bio" style="margin-top: 10px;">
											I am Guy, I’m a PhD student with a deep interest in exploring the intersections of sensory ecology, movement behaviour, and neurobiology. My work extends encompassing diverse projects such as bioacoustics of moths, electrophysiological studies of ascidians, and sensory ecology investigations of whirling beetles. Alongside my research, I find immense fulfillment in teaching and mentoring. 
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> My current project focuses on the remarkable adaptations of whirling beetles (Coleoptera: Gyrinidae), which navigate the surface tension of freshwater ecosystems using dual visual systems and highly sensitive antennae. Early hypotheses suggest these beetles rely on returning surface waves for spatial orientation, but this has yet to be thoroughly tested with modern tools. By employing high-speed imaging and computer vision algorithms, my research aims to track these surface waves and analyse beetle behaviour in response to them. 
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_fonda.png" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Kalindi Fonda</h4>
										<i></i>

										<p class="bio" style="margin-top: 10px;">
											I’ve been crafting my tech for nature journey over the last few years, sometimes more tech, sometimes more nature and sometimes more journey.
											My experience is a collage of projects, roles and learning that have given me valuable insights into the tech for nature space. I’ve worked on placing underwater cameras to monitor mantas, filmed stories for a documentary about a wildlife reserve in Namibia, and contributed to an ocean bioacoustics project as my 20% engagement while at Google.
											Currently, I am building Nature Venture, an organisation to accelerate nature projects. I also collaborate with Wildlife.ai and the New Zealand Department of Conservation on a marine reserve and fish monitoring via BUV (Baited Underwater Videos) data and ML project. 
											I worked in ed-tech for a big part of the last decade, because of its mission of making education and knowledge accessible, and I am still involved by sharing and giving talks on the ways we can use tech for good especially thanks to its leverage and power to scale.
											I crossed the Atlantic Ocean in a sailboat at the beginning of last year, where I wrote most of my thesis The Role of Biodiversity in Business. I love learning. I cycle and play underwater rugby.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Two years ago I spent some time at LAMAVE's Manta research station in Palawan, Philippines. We deployed underwater camera traps, which took a photo every five seconds of areas of interest, be it coral that might be a cleaning station or a feeding corridor. Every couple of days, we would replace the batteries, retrieve the SD cards and return with thousands of photos: 10.000 per camera per day.
											We would then go through these by hand to identify manta ray sightings and other species such as sharks, turtles, and rays. We would also recognize individual mantas based on the unique spot patterns on their bellies.
											Each step of this process was manual, and my CV4E project aims to make this process simpler and more automated: from reducing the amount of "empty" images (those containing nothing but sea and small fish, the images containing animals of interest represented a small portion), identifying species of interest, or even manta individuals. I am excited to see how far we'll get in the 3 weeks of the summer school! 
											Streamlining this process would enable LAMAVE and similar organizations to access research insights more quickly and with fewer resources. This kind of work provides essential data for research (monitoring the health of marine ecosystems, and identifying critical habitats) and supports policy efforts, such as establishing and preserving marine reserves. Additionally, I believe that by showing examples of tech for good we can inspire more people to join this mission! 											
										</p>
									</div> 

								<!--<div class="bio-container">
										<span class="image left">
											<img src="images/student_alksne.jpeg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Michaela Alksne</h4>
										<i>Scripps Institution of Oceanography </i>

										<p class="bio" style="margin-top: 10px;">
											Michaela (she/her/hers) is a PhD student and NDSEG Fellow at Scripps Institution of Oceanography. Her dissertation research focuses on applying novel machine learning tools to improve underwater acoustics data processing. Trained as a marine mammal biologist, Michaela’s core research goals are to understand cetacean (whales, dolphins, and porpoises) behavior, distribution, and foraging ecology, using acoustics. To do just this, Michaela is excited about applying computer vision tools to detect and classify unique cetacean signal types in spectrograms. When she’s not listening to whale songs or wrestling matlab and python, you can usually find her surfing or swimming nearby. 
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Blue whales come to Southern California in the summer to forage on dense patches of microscopic krill. While in the region, they produce low-frequency foraging, social, and reproductive calls. Using bottom-moored acoustic sensors, we have a long-term record of their singing behavior at multiple sights, giving us insight into their seasonal, diel, and interannual behavioral patterns. However, given the large volumes of data we collect, detection of blue whale songs is like finding needles in a haystack. There is currently no reliable method to detect and classify highly variable blue whale social and foraging calls. For CV4Ecology, Michaela is bringing a training dataset of blue whale call spectrograms in an effort to develop a robust deep learning model that is capable of detecting and classifying complex, highly variable, northeastern pacific blue whale calls in passive acoustic data. 
										</p>
									</div> 

 									<div class="bio-container">
										<span class="image left">
											<img src="images/student_williams.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Ben Williams</h4>
										<i>University College London and Zoological Society of London</i>

										<p class="bio" style="margin-top: 10px;">
											Hi I’m Ben, a PhD student at UCL and ZSL in the UK, supported by the Fisheries Society of the British Isles. My PhD is focused on leveraging machine-learning to monitor and support coral reef conservation. This is in close cooperation with my industry partner Mars, who operate the world’s largest coral reef restoration program (buildingcoral.com). I have a strong background in marine biology and have enjoyed the continued opportunity to boost my skillset in machine learning over the course of my PhD. 
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> The soundscape of coral reefs is a rich and diverse symphony of different noises. An increasing body of research has shown there is much we can learn about the state of these habitats by listening. Machine learning presents the cutting edge in similar domains, but is so far underutilized on reef soundscapes. My project aims to utilize a large and diverse dataset of reef soundscape recordings from across the tropics to develop a first prototype of a foundational model that can be used as a springboard for ML led analysis of reef acoustic data. This model should enable users to extract domain specific feature embeddings from reef audio, enabling the training of unsupervised clustering algorithms or shallow classifiers with most of the time and compute cost removed. 
										</p>
									</div>
							
<!--							                <div class="bio-container">
										<span class="image left">
											<img src="images/student_prybyla.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Alixandra Prybyla (<a href="https://twitter.com/EdiBeeBrigade" target="_blank">@EdiBeeBrigade</a>)</h4>
										<i>University of Edinburgh</i>

										<p class="bio" style="margin-top: 10px;">
											Alixandra is a PhD student and Darwin Trust Scholar at the University of Edinburgh. As an undergraduate student at Columbia University, Alix developed a passion for non-lethal and non-invasive survey sampling methods, thus bringing her to her current work in acoustic monitoring. Her doctoral work focuses on how the flight sounds of the humble bumblebee can be used as a way to assess population dynamics–an especially pressing line of inquiry given that bumblebees are facing worldwide declines, and are important bioindicators. Alix is a firm believer that interdisciplinary collaboration can change the landscape of conservation ecology for the better, and she is excited to be an explorer of that horizon. She enjoys fire spinning and riding ponies across the Scottish moorlands in her spare time. 
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Bumblebees face global population declines. As important pollinators, ongoing monitoring of population size, geographic distribution, and demography (i.e. age, sex, size) is essential for risk assessment and conservation. Because insect surveys are labor-intensive, automated and remote survey techniques--like acoustic monitoring--are of increasing interest. Bumblebees are excellent candidates for remote acoustic monitoring due to their distinct flight, foraging, and behavioral sounds. Therefore, the goal of this summer research is to train an AI-based verification system that can identify species-diagnostic signatures in a continuous stream of real-time sounds. Such technology would be a boon to people in the agricultural sectors, conservationists, and hobbyists alike.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_renne.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Rachel Renne</h4>
										<i>Yale School of the Environment</i>

										<p class="bio" style="margin-top: 10px;">
											Rachel grew up in rural southwest Florida, in the heart of orange and cattle-country. After graduating from college in 2008, she began a three-mile-per-hour tour—on foot—of the subtle and dramatic shifts of vegetation across the American landscape, hiking over 10,500 miles on National Scenic Trails. Rachel is a doctoral student at the Yale School of the Environment. She is fascinated by how soils translate climate into vegetation in drylands, and her favorite drylands are big sagebrush ecosystems. She is excited by the potential for combining traditional field data with artificial intelligence to answer fundamental and applied questions about plant community ecology.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> In rangelands, gradients of grazing intensity and disturbance form around water sources, with the greatest (and most visible) impacts close to the source, termed the “piosphere”. Ranching practices draw upon a suite of attractants (including artificial water sources) to achieve more even use of forage across the landscape. Yet, data sources for these attractants are incomplete and inaccurate. My project will use computer vision to detect piospheres in Wyoming using aerial imagery. This project will provide critical data to identify water- and livestock-remote areas, with important applications for land management, conservation, and sample design for research (like mine) that addresses fundamental questions about the interacting roles of environmental factors and disturbance in determining dryland plant community structure and function.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_rustemeyer.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Felix Rustemeyer (<a href="https://twitter.com/FelixRustemeyer" target="_blank">@FelixRustemeyer</a>)</h4>
										<i>Maastricht University / Stockholm Environment Institute</i>

										<p class="bio" style="margin-top: 10px;">
											I am Felix Rustemeyer, a dutch-german graduate student from Maastricht University in the Netherlands. After conducting a bachelors in Artificial Intelligence at the University of Amsterdam, I am now finishing my masters in Data Science for Decision Making. Inspired by a minor in Geomatics during an exchange in Sweden, I have grown a passion for employing the power of deep learning and computer vision for raising awareness and tackling the issues of our time. In an internship role at the Stockholm Environment Institute, I have conducted this last semester by using deep learning for identifying the impact of permafrost thaw. I am honored to be part of the inaugural cohort of the CV4Ecology summer school and am extremely excited for August!
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Thawing permafrost is a huge problem as permafrost soils are heavily carbon rich and is therefore causing large amounts of GHG emissions, while being absent from many international dialogues and strategies aimed at tackling climate change. The Stockholm Environment Institute has established a cross-sector collaboration to fill in this knowledge gap. The goal of this project is to identify landscape indications of thawing permafrost in the arctic using computer vision and satellite imagery. By identifying and monitoring these landscape indications in the form of thaw slumps, the extent, increase and the impact on climate caused by thawing permafrost in the arctic can be estimated.
										</p>
									</div>
							               
							                <div class="bio-container">
										<span class="image left">
											<img src="images/student_sakai.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Taiki Sakai</h4>
										<i>Environmental Assessment Services / SWFSC Acoustic Ecology Lab</i>

										<p class="bio" style="margin-top: 10px;">
											Taiki is a contractor at NOAA’s Southwest Fisheries Science Center working in the Acoustic Ecology Lab. There he works with researchers who use passive acoustic monitoring to study marine mammals off the coast of California. His main focus is building free, open-source software tools that make it easier to analyze passive acoustic data. His favorite animals are beaked whales and frogfish because they are adorable weirdos. When he’s not behind a computer, you can find him hiking, climbing rocks, or playing with his dog, Noodle. 
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> Beaked whales are deep diving marine mammals that are difficult to study using traditional visual methods because of how little time they spend at the surface. During their long foraging dives they make echolocation clicks which can be uniquely identified to different beaked whale species, which makes them an ideal candidate to study using passive acoustics. In this project we will create images of echolocation clicks using the Wigner-Ville transform and attempt to classify them to species.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_shafron.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Ethan Shafron</h4>
										<i>University of Montana Spatial Analysis Lab</i>

										<p class="bio" style="margin-top: 10px;">
											I am an analyst and field coordinator for the University of Montana Spatial Analysis Lab in. After earning my Bachelors degree in Environmental Studies from University of Vermont in 2019, I worked on geospatial scientific computing and imaging spectroscopy at Arizona State University. My work there involved mapping coral reefs, foliar functional trait mapping, and survey design for marine biodiversity monitoring. I then spent a winter season working with Glacier National Park's Citizen Science program helping to create wildlife surveys and analytical products for improving park management and visitor experience. I am interested in spatial ecology, land system science, algorithm development, and machine learning.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> We are working to characterize woodland draws and ravines across a huge area of Eastern Montana and the Dakotas in conjunction with the Bureau of Land Management. These systems are critical for wildlife and grazing, and are vital to the health of the region and function of its agriculture industry. These systems are particularly difficult to monitor due to their narrow, dendritic shape, so we will be using computer vision methods and remote sensing to develop a scalable monitoring protocol.
										</p>
									</div>

									<div class="bio-container">
										<span class="image left">
											<img src="images/student_youngflesh.jpg" alt="" width="50" />
										</span>

										<h4 style="margin-bottom: -2px;">Casey Youngflesh (<a href="https://twitter.com/CaseyYoungflesh" target="_blank">@CaseyYoungflesh</a>)</h4>
										<i>Michigan State University (as of Aug 1, 2022)</i>

										<p class="bio" style="margin-top: 10px;">
											I am a quantitative ecologist and Presidential Postdoc Fellow in EEB at Michigan State University (as of August 2022). My research uses a range of quantitative approaches that synthesize large-scale data streams to address both basic and applied questions in population and community ecology. I seek to understand how and why ecological systems are responding to rapid climatic change and what this might tell us regarding how best to conserve these systems. I have a particular interest in the timing of seasonal events (phenology), population dynamics, and biodiversity.
										</p>

										<p class="bio" style="margin-top: 10px;">
											<b>Project:</b> The Pacific walrus is an important benthic predator in Arctic marine ecosystems and a species of substantial cultural and economic importance to many Indigenous Arctic coastal communities. As a result of increasing pressures associated with global change, the future of this species is highly uncertain. I aim to develop a deep learning-based classifier to identify walrus in satellite imagery. This will serve as the basis for large-scale, automated efforts to monitor the distribution and abundance of this species and will provide stakeholders with information needed to make timely conservation management decisions.
										</p>
									</div>

									<hr />-->

								</section>
						</div>
					</div>

				<!-- Sidebar -->
										<div id="sidebar">
						<div class="inner">

							<!-- Search -->
<!-- 								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section> -->

							<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Homepage</a></li>
									<li><a>Course Materials</a></li>
									<ul>
									<li><a href="course_details.html">Overview</a></li>
									<li><a href="course_content2023.html">Lectures and Syllabus</a></li>
									</ul>
									<li><a>People</a></li>
									<ul>
									<li><a href="students2025.html">Students</a></li>
									<li><a href="people2025.html">Instructors</a></li> 
									<li><a href="speakers2023.html">Speakers</a></li>
									</ul>
									<li><a href="important_dates.html">Important Dates</a></li>
									<li><a href="faq.html">FAQ</a></li>
									<li><a href="call_for_applications.html">Apply</a></li>

									<!--
									<li><a>2022</a></li>
									<ul>
									<li><a href="students2022.html">Students</a></li>
									<li><a href="speakers2022.html">Speakers</a></li>
									<li><a href="people2022.html">Instructors</a></li>
									<li><a href="course_content2022.html">Course Materials</a></li>
									</ul>
									<li><a>2023</a></li>
									<ul>
									<li><a href="students2023.html">Students</a></li>
									<li><a href="speakers2023.html">Speakers</a></li>
									<li><a href="people2023.html">Instructors</a></li>
									<li><a href="course_content2023.html">Course Materials</a></li>
									</ul>
									<li><a>2025</a></li>
									<ul>
									<li><a href="students2025.html">Students</a></li>
									<li><a href="speakers2025.html">Speakers</a></li>
									<li><a href="people2025.html">Instructors</a></li> 
									<li><a href="course_content2025.html">Course Materials</a></li>
									</ul>-->

									<!-- <li><a href="https://docs.google.com/spreadsheets/d/1IP4DWrmkhN5zny4ljq83HwBRuKe7gH3hDZSMXn8d__0/edit?usp=sharing">Schedule</a></li> -->
									<!--
										<li>
										<span class="opener">Apply</span>
										<ul>
											<li><a href="call_for_applications.html">Call for Applications</a></li>

											<li><a href="#">Application Portal</a></li>
										</ul>
									</li>
									-->
								</ul>
							</nav>
							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="mailto:cv4ecology@caltech.edu">cv4ecology@caltech.edu</a></li>
										<li class="icon brands fa-twitter"><a href="https://twitter.com/cv4ecology" target="_blank">@cv4ecology</a></li>
										<li class="icon brands fa-github"><a href="https://github.com/cv4ecology" target="_blank">contribute on GitHub</a></li>
										<li class="icon solid fa-envelope"><a href="https://forms.gle/WAD76JceCW9osDm97" target="_blank">Sign up for our mailing list!</a></li>
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright"> This material is based upon work supported by the National Science Foundation under Award No. 2330423. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
									<p class="copyright">&copy; California Institute of Technology.<br/>All rights reserved.<br/>Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>


		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
